optimizer:
  desc: Namespace for configuring optimization of `model` parameters
  name:
    desc: What optimizer to use from the `optax` namespace
    value: adamw
  learning_rate:
    desc: Initial learning rate for the optimizer
    value: 1.0e-3
  l2_weight_decay:
    desc: l2 prior variance on the neural network weights
    value: 1.0e-6
  max_grad_norm:
    desc: Maximum gradient norm for clipping
    value: 1.0
  max_grad:
    desc: Maximum absolute value for any gradient element
    value: 5.0

loss:
  desc: Namespace for configuring how to optimize `model` parameters
  elbo:
    desc: Specify which Evidence Lower BOund implementation to optimize.
    value: VariBADELBO
  elbo_kwargs:
    desc: ELBO implementation specific hyperparameters
    value: { window_size: 10, beta: 0.01, num_model_samples: 1 }
  target_loss:
    desc: Specify which loss function implementation to use
    value:
      - cross_entropy.EmpiricalCrossEntropy
  target_loss_kwargs:
    desc: Target Loss specific hyperparameters
    value:
      - { modality: 'Signal y_hat' }
  target_weights:
    desc: Weight per Loss
    value:
      - 1.0
  ignore_model_complexity:
    desc: Whether the model should drop the KL-penalty in optimization.
    value: False
  simplify_model_complexity:
    desc: Whether to simplify the Gaussian KL to the Mahalanobis Distance.
    value: False

learner:
  desc: Namespace for configuring the Learning/ Consumer algorithm
  value: { }
